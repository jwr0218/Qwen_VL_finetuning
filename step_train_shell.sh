#!/bin/bash
export CODECARBON_LOG_LEVEL="error"
export CODECARBON_DISABLED="true"  # íƒ„ì†Œ ì¸¡ì •ì´ í•„ìš” ì—†ë‹¤ë©´ ì•„ì˜ˆ ë„ê¸°

# --- ê¸°ë³¸ ì„¤ì • ---
BASE_MODEL="huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated"
DATA_PATH="/workspace/Toonspace_VLM/data/ocr_description/total(bbox_normal)_ocr_dataset_2F.json"
PROJECT_NAME="qwen3-Native-Stepped-Training"
OUTPUT_ROOT="ex_models/native_stepped"

# --- Stepë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---

# [Step 1] Internal Adapter: ì´ˆê¸° ì •ë ¬ì„ ìœ„í•´ 1:1 ë¹„ìœ¨ ì‚¬ìš©
STEP1_LR=1e-4
STEP1_TEXT_WEIGHT=1.0
STEP1_BBOX_WEIGHT=1.0

# [Step 2] Decoder: í…ìŠ¤íŠ¸ ìƒì„± ëŠ¥ë ¥ ê°•í™” (ê¸°ì¡´ ì„¤ì • ë³µê·€)
STEP2_LR=2e-5
STEP2_TEXT_WEIGHT=2.0
STEP2_BBOX_WEIGHT=0.1

# [Step 3] Full Fine-tuning: ì „ì²´ ìµœì í™”
STEP3_LR=5e-6
STEP3_TEXT_WEIGHT=2.0
STEP3_BBOX_WEIGHT=0.1

# ê³µí†µ ì„¤ì •
BATCH_SIZE=1
ACCUM_STEPS=4

# ì—ëŸ¬ ë°œìƒ ì‹œ ì¤‘ë‹¨
set -e

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ì˜
STEP1_DIR="${OUTPUT_ROOT}/step1_internal_adapter"
STEP2_DIR="${OUTPUT_ROOT}/step2_decoder"
STEP3_DIR="${OUTPUT_ROOT}/step3_full"

# echo "======================================================"
# echo "ğŸš€ Step 1: Internal Adapter Only (Weight ${STEP1_TEXT_WEIGHT}:${STEP1_BBOX_WEIGHT})"
# echo "======================================================"

# accelerate launch --multi_gpu --num_processes=2 train/step_train.py \
#     --model_id "$BASE_MODEL" \
#     --processor_id "$BASE_MODEL" \
#     --data_path "$DATA_PATH" \
#     --output_dir "$STEP1_DIR" \
#     --stage "step1" \
#     --wandb_project "$PROJECT_NAME" \
#     --wandb_name "step1_merger_only" \
#     --num_train_epochs 3 \
#     --learning_rate $STEP1_LR \
#     --per_device_train_batch_size $BATCH_SIZE \
#     --gradient_accumulation_steps $ACCUM_STEPS \
#     --text_weight $STEP1_TEXT_WEIGHT \
#     --bbox_weight $STEP1_BBOX_WEIGHT \

# echo "âœ… Step 1 Completed."
# sleep 60

# echo "======================================================"
# echo "ğŸš€ Step 2: Decoder Training (Weight ${STEP2_TEXT_WEIGHT}:${STEP2_BBOX_WEIGHT})"
# echo "======================================================"

# accelerate launch --multi_gpu --num_processes=2 train/step_train.py \
#     --model_id "$BASE_MODEL" \
#     --processor_id "$BASE_MODEL" \
#     --data_path "$DATA_PATH" \
#     --resume_from_prev_stage "$STEP1_DIR" \
#     --output_dir "$STEP2_DIR" \
#     --stage "step2" \
#     --wandb_project "$PROJECT_NAME" \
#     --wandb_name "step2_decoder_ft" \
#     --num_train_epochs 5 \
#     --learning_rate $STEP2_LR \
#     --per_device_train_batch_size $BATCH_SIZE \
#     --gradient_accumulation_steps $ACCUM_STEPS \
#     --text_weight $STEP2_TEXT_WEIGHT \
#     --bbox_weight $STEP2_BBOX_WEIGHT \

# echo "âœ… Step 2 Completed."
# sleep 60

echo "======================================================"
echo "ğŸš€ Step 3: Full Fine-tuning (Weight ${STEP3_TEXT_WEIGHT}:${STEP3_BBOX_WEIGHT})"
echo "======================================================"

accelerate launch --multi_gpu --num_processes=2 train/step_train.py \
    --model_id "$BASE_MODEL" \
    --processor_id "$BASE_MODEL" \
    --data_path "$DATA_PATH" \
    --resume_from_prev_stage "$STEP2_DIR" \
    --output_dir "$STEP3_DIR" \
    --stage "step3" \
    --wandb_project "$PROJECT_NAME" \
    --wandb_name "step3_full_ft" \
    --num_train_epochs 7 \
    --learning_rate $STEP3_LR \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --text_weight $STEP3_TEXT_WEIGHT \
    --bbox_weight $STEP3_BBOX_WEIGHT \

echo "ğŸ‰ All Steps Completed Successfully!"