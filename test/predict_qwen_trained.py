"""
Vision Language Model ì˜ˆì¸¡ ë° í‰ê°€ í†µí•© ìŠ¤í¬ë¦½íŠ¸ (ì›¹íˆ° ìˆœì°¨ ì¶”ë¡  ë° OCR í‰ê°€ ì§€ì›)

ì´ ëª¨ë“ˆì€ íŒŒì¸íŠœë‹ëœ Qwen2.5-VL ëª¨ë¸ì˜ ì˜ˆì¸¡ ë° í‰ê°€ë¥¼ ë‹¨ì¼ í´ë˜ìŠ¤ ë‚´ì—ì„œ êµ¬í˜„í•©ë‹ˆë‹¤.
'predict' ëª¨ë“œì—ì„œëŠ” ì›¹íˆ° ì´ë¯¸ì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¶„ì„í•˜ë©° ì»¨í…ìŠ¤íŠ¸ë¥¼ ëˆ„ì  ì²˜ë¦¬í•˜ê³ ,
'evaluate' ëª¨ë“œì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± ë° OCR/Detection ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

ì‘ì„±ì: Assistant
Year: 2025
Month: 9
Day: 17
(Refactored on: 2025-10-16)
"""
import gc
import json
import logging
import os
import re
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
from datasets import load_dataset
from nltk import download, word_tokenize
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from PIL import Image
from qwen_vl_utils import process_vision_info
# í‰ê°€ ì§€í‘œ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from rouge import Rouge
from bert_score import score as bert_score
from tqdm import tqdm
from transformers import (AutoProcessor,
                          Qwen2_5_VLForConditionalGeneration)

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
try:
    download('punkt', quiet=True)
    download('wordnet', quiet=True)
except Exception:
    pass

@dataclass
class VLMConfig:
    """VLM ì˜ˆì¸¡ ë° í‰ê°€ ì„¤ì •ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ë°ì´í„°í´ë˜ìŠ¤"""
    
    # ğŸ“ ë°ì´í„° ë° ëª¨ë¸ ê²½ë¡œ
    model_path: str = ''
    base_model_id: str = "Qwen/Qwen2-VL-7B-Instruct" 

    # â¡ï¸ ì˜ˆì¸¡ ëª¨ë“œ ì„¤ì •
    image_folder: str = '/workspace/Toonspace_VLM/webtoon_images'
    use_previous_context: bool = False
    max_context_length: int = 2000
    context_summary_length: int = 500
    image_extensions: List[str] = field(default_factory=lambda: ['.jpg', '.jpeg', '.png', '.webp'])
    # base_prompt: str = "ì´ ì›¹íˆ° ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ëŒ€ì‚¬, íš¨ê³¼ìŒ, ìºë¦­í„° í–‰ë™, ê°ì •ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
#     context_prompt_template: str = """ì´ì „ ì¥ë©´ ìš”ì•½:
# {previous_context}

# í˜„ì¬ ì´ë¯¸ì§€ë¥¼ ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”. ì´ ì›¹íˆ° ì´ë¯¸ì§€ì˜ ëŒ€ì‚¬, íš¨ê³¼ìŒ, ìºë¦­í„° í–‰ë™, ê°ì •ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""

    # ğŸ“Š í‰ê°€ ëª¨ë“œ ì„¤ì •
    test_data_path: str = '/workspace/Toonspace_VLM/test/OCR_test_dataset.json'
    compute_bertscore: bool = True
    compute_json_metrics: bool = True
    
    ##### NEW/MODIFIED START #####
    compute_ocr_metrics: bool = True # OCR/Detection ë©”íŠ¸ë¦­ ê³„ì‚° ì—¬ë¶€
    iou_threshold: float = 0.5       # IoU ì„ê³„ê°’
    ##### NEW/MODIFIED END #####

    # âš™ï¸ ëª¨ë¸ ë° ìƒì„± ê³µí†µ ì„¤ì •
    output_dir: str = 'results'
    predictions_file: str = 'predictions.json'
    metrics_file: str = 'evaluation_metrics.json'
    batch_size: int = 1
    max_samples: Optional[int] = None
    max_new_tokens: int = 1024
    temperature: float = 0.7
    top_p: float = 0.9
    do_sample: bool = True
    min_pixels: int = 256 * 28 * 28
    max_pixels: int = 960 * 28 * 28

    # ğŸ“ ì‹œìŠ¤í…œ ë©”ì‹œì§€
    system_message: str = field(default="""
    ë‹¹ì‹ ì€ ì›¹íˆ° ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›¹íˆ°ì˜ ì—°ì†ëœ ì¥ë©´ì„ ë¶„ì„í•˜ì—¬ ìŠ¤í† ë¦¬ì˜ íë¦„ì„ ì´í•´í•˜ê³ ,
    ê° ì¥ë©´ì˜ ëŒ€ì‚¬, íš¨ê³¼ìŒ, ìºë¦­í„°ì˜ í–‰ë™ê³¼ ê°ì •, ì„œì‚¬ì  ë§¥ë½ì„ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì´ì „ ì¥ë©´ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì¥ë©´ì„ ë” ì •í™•í•˜ê²Œ í•´ì„í•˜ê³ ,
    ëª¨ë“  ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.
    """)

class WebtoonVLM:
    """ì›¹íˆ° VLM ì˜ˆì¸¡ ë° í‰ê°€ë¥¼ ìœ„í•œ í†µí•© í´ë˜ìŠ¤"""

    def __init__(self, config: VLMConfig):
        self.config = config
        self.setup_logging()
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.processor = None
        self.rouge = Rouge()
        
        self.load_model_and_processor()

    def setup_logging(self) -> None:
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(Path(self.config.output_dir) / 'vlm_runner.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def load_model_and_processor(self) -> None:
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ"""
        if self.model is not None:
            self.logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        try:
            self.logger.info(f"ëª¨ë¸ ë¡œë“œ ì‹œì‘: {self.config.model_path}")
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                self.config.model_path, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True,
            )
            self.processor = AutoProcessor.from_pretrained(
                self.config.base_model_id, min_pixels=self.config.min_pixels, max_pixels=self.config.max_pixels, trust_remote_code=True
            )
            self.model.eval()
            self.logger.info("ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def clear_memory(self) -> None:
        """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
    def run(self, mode: str):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜: ëª¨ë“œì— ë”°ë¼ ì˜ˆì¸¡ ë˜ëŠ” í‰ê°€ ì‹¤í–‰"""
        if mode == 'predict':
            return self._run_prediction()
        elif mode == 'evaluate':
            return self._run_evaluation()
        else:
            self.logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œì…ë‹ˆë‹¤: {mode}. 'predict' ë˜ëŠ” 'evaluate'ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            raise ValueError(f"Invalid mode: {mode}")
    
    # ... (ì´ì „ ì½”ë“œì˜ ì˜ˆì¸¡ ëª¨ë“œ ê´€ë ¨ ë©”ì„œë“œë“¤ì€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ìƒëµ) ...
    def _run_prediction(self) -> List[Dict[str, Any]]:
        folder_path = self.config.image_folder
        self.logger.info(f"í´ë” ì˜ˆì¸¡ ëª¨ë“œ ì‹œì‘: {folder_path}")
        image_files = self._get_sorted_image_files(folder_path)
        if not image_files: return []
        all_results, context_history = [], []
        for idx, image_path in enumerate(tqdm(image_files, desc="Processing images")):
            prompt = self._build_context_prompt(context_history)
            result = self._predict_single(image_path=image_path, query=prompt)
            result['sequence_number'] = idx + 1
            result['used_context'] = self.config.use_previous_context and bool(context_history)
            if self.config.use_previous_context:
                context_summary = self._extract_key_info_from_prediction(result['prediction'])
                context_history.append(context_summary)
                if len(context_history) > 5: context_history.pop(0)
            all_results.append(result)
            if (idx + 1) % 10 == 0: self.clear_memory()
        self._save_prediction_results(all_results, folder_path)
        self.logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ: ì´ {len(all_results)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬")
        return all_results
    def _get_sorted_image_files(self, folder_path: str) -> List[Path]:
        folder = Path(folder_path)
        if not folder.is_dir(): raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        image_files = [p for p in folder.iterdir() if p.suffix.lower() in self.config.image_extensions]
        def natural_sort_key(path): return [int(c) if c.isdigit() else c.lower() for c in re.split(r'(\d+)', path.name)]
        image_files.sort(key=natural_sort_key)
        self.logger.info(f"ì´ {len(image_files)}ê°œì˜ ì´ë¯¸ì§€ íŒŒì¼ ë°œê²¬")
        return image_files
    def _build_context_prompt(self, context_history: List[str]) -> str:
        if not self.config.use_previous_context or not context_history: return self.config.base_prompt
        context_summary = "\n".join([f"ì¥ë©´ {i+1}: {ctx}" for i, ctx in enumerate(context_history)])
        if len(self.processor.tokenizer.encode(context_summary)) > self.config.max_context_length:
            context_summary = context_summary[-self.config.max_context_length:]
        return self.config.context_prompt_template.format(previous_context=context_summary)
    def _extract_key_info_from_prediction(self, prediction: str) -> str:
        try:
            pred_json = json.loads(prediction)
            summary_parts = []
            if 'dialogues' in pred_json and pred_json['dialogues']: summary_parts.append(f"ëŒ€ì‚¬: {', '.join([d.get('text', '') for d in pred_json['dialogues'][:2]])}")
            if 'characters' in pred_json and pred_json['characters']: summary_parts.append(f"í–‰ë™: {', '.join([c.get('action', '') for c in pred_json['characters'][:2] if c.get('action')])}")
            return ' | '.join(summary_parts) if summary_parts else prediction[:self.config.context_summary_length]
        except (json.JSONDecodeError, TypeError): return prediction[:self.config.context_summary_length]
    def _save_prediction_results(self, results: List[Dict], source_folder: str):
        output_data = {'metadata': {'source_folder': source_folder, 'model_path': self.config.model_path, 'total_images': len(results), 'timestamp': datetime.now().isoformat(), 'config': self.config.__dict__}, 'predictions': results}
        output_path = Path(self.config.output_dir) / self.config.predictions_file
        with open(output_path, 'w', encoding='utf-8') as f: json.dump(output_data, f, ensure_ascii=False, indent=2)
        pd.DataFrame(results).to_csv(Path(self.config.output_dir) / 'predictions.csv', index=False, encoding='utf-8-sig')
        self.logger.info(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    # ===================================================================
    # í‰ê°€ ëª¨ë“œ (Evaluation Mode) - ìˆ˜ì •ë¨
    # evaluate - query : OCR ë§Œ ì‚¬ìš©. 
    # ===================================================================

    def _run_evaluation(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€ ì§€í‘œ ê³„ì‚°"""
        self.logger.info("í‰ê°€ ëª¨ë“œ ì‹œì‘...")
        test_data = self._load_test_data()
        results_with_metadata = []
        
        start_time = time.time()
        for idx, sample in enumerate(tqdm(test_data, desc="Evaluating")):
            try:
                query = sample['query']
                result = self._predict_single(image_path=sample['image_path'], query=query)
                result['ground_truth'] = sample['answer']
                results_with_metadata.append(result)
            except Exception as e:
                self.logger.error(f"ìƒ˜í”Œ {idx+1} ({sample.get('image_path')}) ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                results_with_metadata.append({"image_path": sample.get('image_path'), "prediction": "[ERROR]", "ground_truth": sample.get('answer'), "error": str(e)})
            if (idx + 1) % 10 == 0: self.clear_memory()

        total_time = time.time() - start_time
        self.logger.info(f"í‰ê°€ ì™„ë£Œ. ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")

        predictions = [r['prediction'] for r in results_with_metadata]
        ground_truths = [r['ground_truth'] for r in results_with_metadata]
        
        metrics = self._calculate_all_metrics(predictions, ground_truths)
        self._save_evaluation_results(metrics, results_with_metadata)
        
        return metrics

    def _load_test_data(self) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            dataset = load_dataset('json', data_files=self.config.test_data_path)['train']
            # dataset = dataset.select(range(20))
            if self.config.max_samples:
                dataset = dataset.select(range(min(self.config.max_samples, len(dataset))))
            self.logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ ìƒ˜í”Œ")
            return dataset
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    ##### NEW/MODIFIED START #####
    def _parse_ocr_string(self, ocr_str: str) -> List[Dict[str, Union[str, List[float]]]]:
        """ 'TEXT : [x1, y1, x2, y2]' í˜•ì‹ì˜ ë¬¸ìì—´ì„ íŒŒì‹±í•©ë‹ˆë‹¤. """
        entries = []
        if not isinstance(ocr_str, str):
            return []
        for line in ocr_str.strip().split('\n'):
            try:
                parts = line.split(' : ')
                if len(parts) == 2:
                    text = parts[0]
                    # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ íŒŒì‹±
                    bbox_str = parts[1].strip()
                    bbox = json.loads(bbox_str)
                    if len(bbox) == 4:
                        entries.append({'text': text, 'box': [float(coord) for coord in bbox]})
            except (json.JSONDecodeError, IndexError, ValueError) as e:
                self.logger.debug(f"OCR ë¬¸ìì—´ íŒŒì‹± ì‹¤íŒ¨: '{line}'. ì˜¤ë¥˜: {e}")
                continue
        return entries

    def _calculate_iou(self, boxA: List[float], boxB: List[float]) -> float:
        """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ ê°„ì˜ IoU(Intersection over Union)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        xA = max(boxA[0], boxB[0])
        yA = max(boxA[1], boxB[1])
        xB = min(boxA[2], boxB[2])
        yB = min(boxA[3], boxB[3])

        interArea = max(0, xB - xA) * max(0, yB - yA)
        boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
        boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
        
        unionArea = float(boxAArea + boxBArea - interArea)
        return interArea / unionArea if unionArea > 0 else 0.0
    def _compute_map(self, predictions: List[str], ground_truths: List[str], iou_threshold: float) -> float:
        """
        ë°ì´í„°ì…‹ ì „ì²´ì— ëŒ€í•œ Average Precision (AP)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì°¸ê³ : ì‹ ë¢°ë„ ì ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ ëª¨ë“  ì˜ˆì¸¡ì˜ ì‹ ë¢°ë„ë¥¼ 1.0ìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
        """
        all_preds = []
        total_gt_boxes = 0

        # 1. ëª¨ë“  ì˜ˆì¸¡ê³¼ ì •ë‹µì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
        for i, (pred_str, gt_str) in enumerate(zip(predictions, ground_truths)):
            pred_boxes = self._parse_ocr_string(pred_str)
            gt_boxes = self._parse_ocr_string(gt_str)
            total_gt_boxes += len(gt_boxes)
            
            # ê° ì˜ˆì¸¡ì— ì´ë¯¸ì§€ ì¸ë±ìŠ¤ì™€ ì‹ ë¢°ë„(ê°€ì§œ) ì¶”ê°€
            for pred_box in pred_boxes:
                all_preds.append({'image_id': i, 'confidence': 1.0, 'box': pred_box})
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (í˜„ì¬ëŠ” ëª¨ë‘ 1.0ì´ë¼ ì˜ë¯¸ëŠ” ì—†ì§€ë§Œ, ì•Œê³ ë¦¬ì¦˜ìƒ í•„ìš”)
        all_preds.sort(key=lambda x: x['confidence'], reverse=True)

        # 2. ê° ì˜ˆì¸¡ì— ëŒ€í•´ TP/FP íŒë³„
        gt_matched_map = {i: [False] * len(self._parse_ocr_string(gt_str)) for i, gt_str in enumerate(ground_truths)}
        
        tp_fp_list = []
        for pred in all_preds:
            image_id = pred['image_id']
            gt_boxes = self._parse_ocr_string(ground_truths[image_id])
            
            best_iou = 0
            best_gt_idx = -1
            for i, gt_box in enumerate(gt_boxes):
                if not gt_matched_map[image_id][i]:
                    iou = self._calculate_iou(pred['box']['box'], gt_box['box'])
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = i

            if best_gt_idx != -1 and best_iou >= iou_threshold and \
               pred['box']['text'].strip() == gt_boxes[best_gt_idx]['text'].strip():
                gt_matched_map[image_id][best_gt_idx] = True
                tp_fp_list.append(1)  # True Positive
            else:
                tp_fp_list.append(0)  # False Positive

        # 3. Precision-Recall ê³¡ì„  ê³„ì‚°
        if total_gt_boxes == 0:
            return 0.0
            
        tp_cumsum = np.cumsum(tp_fp_list)
        precision_curve = [tp_cumsum[i] / (i + 1) for i in range(len(tp_fp_list))]
        recall_curve = [tp_cumsum[i] / total_gt_boxes for i in range(len(tp_fp_list))]

        # 4. AP (Area under curve) ê³„ì‚°
        precision_curve = np.array([0.0] + precision_curve)
        recall_curve = np.array([0.0] + recall_curve)
        
        # ìš°í•˜í–¥ ê³¡ì„ ìœ¼ë¡œ ë³´ì •
        for i in range(len(precision_curve) - 2, -1, -1):
            precision_curve[i] = max(precision_curve[i], precision_curve[i+1])

        # recall ê°’ì´ ë³€í•˜ëŠ” ì§€ì ë§Œ ì„ íƒí•˜ì—¬ ë©´ì  ê³„ì‚°
        recall_change_indices = np.where(recall_curve[1:] != recall_curve[:-1])[0]
        ap = np.sum((recall_curve[recall_change_indices + 1] - recall_curve[recall_change_indices]) * precision_curve[recall_change_indices + 1])
        
        return ap
    def _compute_detection_metrics(self, predictions: List[str], ground_truths: List[str]) -> Dict[str, float]:
        """OCR ë° BBox ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Precision, Recall, F1-score, Avg IoU, mAPë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        total_tp, total_fp, total_fn = 0, 0, 0
        iou_scores_for_tps = [] # TPë“¤ì˜ IoU ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

        for pred_str, gt_str in zip(predictions, ground_truths):
            pred_boxes = self._parse_ocr_string(pred_str)
            gt_boxes = self._parse_ocr_string(gt_str)

            if not gt_boxes and not pred_boxes:
                continue

            tp = 0
            gt_matched = [False] * len(gt_boxes)

            # ê° ì˜ˆì¸¡ ë°•ìŠ¤ë¥¼ ìµœì ì˜ ì •ë‹µ ë°•ìŠ¤ì™€ ë§¤ì¹­
            for pred_box in pred_boxes:
                best_iou = 0
                best_gt_idx = -1
                for i, gt_box in enumerate(gt_boxes):
                    if gt_matched[i]: continue
                    iou = self._calculate_iou(pred_box['box'], gt_box['box'])
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = i

                # TP ì¡°ê±´: IoU ì„ê³„ê°’ ì´ˆê³¼ ë° í…ìŠ¤íŠ¸ ì¼ì¹˜
                if best_gt_idx != -1 and best_iou >= self.config.iou_threshold and \
                   pred_box['text'].strip() == gt_boxes[best_gt_idx]['text'].strip():
                    tp += 1
                    gt_matched[best_gt_idx] = True
                    iou_scores_for_tps.append(best_iou) # TPì¼ ë•Œì˜ IoU ì ìˆ˜ ì €ì¥
            
            fp = len(pred_boxes) - tp
            fn = len(gt_boxes) - sum(gt_matched)

            total_tp += tp
            total_fp += fp
            total_fn += fn

        # --- ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚° ---
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # --- í‰ê·  IoU ê³„ì‚° ---
        average_iou = np.mean(iou_scores_for_tps) if iou_scores_for_tps else 0.0

        # --- mAP ê³„ì‚° ---
        # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ì´ë¯€ë¡œ mAP@0.5ëŠ” AP@0.5ì™€ ë™ì¼í•©ë‹ˆë‹¤.
        map_score = self._compute_map(predictions, ground_truths, self.config.iou_threshold)

        return {
            'ocr_precision': precision,
            'ocr_recall': recall,
            'ocr_f1_score': f1_score,
            'average_iou': average_iou,
            f'mAP_at_{self.config.iou_threshold}': map_score
        }

    def _calculate_all_metrics(self, predictions: List[str], ground_truths: List[str]) -> Dict[str, Any]:
        """ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  í†µí•©"""
        metrics = {}
        valid_pairs = [(p, r) for p, r in zip(predictions, ground_truths) if p and r and p != "[ERROR]"]
        if not valid_pairs:
            self.logger.warning("ê³„ì‚°í•  ìœ íš¨í•œ ì˜ˆì¸¡-ì •ë‹µ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
            return {"error": "No valid prediction-reference pairs found."}
            
        filtered_preds, filtered_refs = zip(*valid_pairs)
        
        # Text Metrics (ê¸°ì¡´)
        try:
            rouge_scores = self.rouge.get_scores(filtered_preds, filtered_refs, avg=True)
            metrics['rouge-l'] = rouge_scores['rouge-l']['f']
        except Exception as e: self.logger.warning(f"ROUGE ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        
        # BERTScore (ê¸°ì¡´)
        if self.config.compute_bertscore:
            try:
                _, _, F1 = bert_score(list(filtered_preds), list(filtered_refs), lang="ko", device=self.device)
                metrics['bert_score_f1'] = F1.mean().item()
            except Exception as e: self.logger.warning(f"BERTScore ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")

        # JSON Metrics (ê¸°ì¡´)
        if self.config.compute_json_metrics:
            # ì´ ë¶€ë¶„ì€ í˜„ì¬ OCR í‰ê°€ì™€ëŠ” ë¬´ê´€í•˜ë¯€ë¡œ, í•„ìš” ì‹œ JSON í˜•ì‹ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
            pass

        # OCR/Detection Metrics (ì‹ ê·œ)
        if self.config.compute_ocr_metrics:
            self.logger.info(f"OCR/Detection ë©”íŠ¸ë¦­ ê³„ì‚° (IoU Threshold: {self.config.iou_threshold})...")
            detection_metrics = self._compute_detection_metrics(predictions, ground_truths)
            metrics.update(detection_metrics)

        return metrics
    ##### NEW/MODIFIED END #####

    def _save_evaluation_results(self, metrics: Dict, results_with_metadata: List[Dict]):
        """í‰ê°€ ê²°ê³¼ ë° ë©”íŠ¸ë¦­ ì €ì¥"""
        metadata = {"timestamp": datetime.now().isoformat(), "model_path": self.config.model_path, "test_data_path": self.config.test_data_path}
        
        metrics_output = {"metadata": metadata, "metrics": metrics}
        metrics_path = Path(self.config.output_dir) / self.config.metrics_file
        with open(metrics_path, 'w', encoding='utf-8') as f: json.dump(metrics_output, f, ensure_ascii=False, indent=4)
        self.logger.info(f"í‰ê°€ ë©”íŠ¸ë¦­ ì €ì¥ ì™„ë£Œ: {metrics_path}")

        predictions_output = {"metadata": metadata, "predictions": results_with_metadata}
        predictions_path = Path(self.config.output_dir) / self.config.predictions_file
        with open(predictions_path, 'w', encoding='utf-8') as f: json.dump(predictions_output, f, ensure_ascii=False, indent=2)
        self.logger.info(f"ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {predictions_path}")

    @torch.no_grad()
    def _predict_single(self, image_path: Union[str, Path], query: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê³µí†µ í•¨ìˆ˜"""
        try:
            image = Image.open(image_path).convert('RGB')
            messages = [{"role": "system", "content": self.config.system_message}, {"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": query}]}]
            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, _ = process_vision_info(messages)
            inputs = self.processor(text=[text], images=image_inputs, padding=True, return_tensors="pt").to(self.device)
            start_time = time.time()
            gen_ids = self.model.generate(**inputs, max_new_tokens=self.config.max_new_tokens, temperature=self.config.temperature, top_p=self.config.top_p, do_sample=self.config.do_sample)
            gen_time = time.time() - start_time
            gen_ids = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen_ids)]
            prediction = self.processor.batch_decode(gen_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
            return {'image_path': str(image_path), 'image_name': Path(image_path).name, 'query': query, 'prediction': prediction, 'generation_time': gen_time}
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ {image_path} ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {'image_path': str(image_path), 'image_name': Path(image_path).name, 'query': query, 'prediction': "[PREDICTION_ERROR]", 'error': str(e)}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='VLM ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€ í†µí•© ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--mode', type=str, default='evaluate', choices=['predict', 'evaluate'])
    parser.add_argument('--model_path', type=str, default='/workspace/Toonspace_VLM/ex_models/at_once_ocr_description')
    parser.add_argument('--base_model_id', type=str, default='huihui-ai/Qwen2.5-VL-7B-Instruct-abliterated')
    parser.add_argument('--image_folder', type=str, default='/workspace/Toonspace_VLM/data/test_image/escape_home/01')
    parser.add_argument('--test_data_path', type=str, default='/workspace/Toonspace_VLM/test/OCR_test_dataset.json')
    parser.add_argument('--output_dir', type=str, default='results')
    parser.add_argument('--use_context', action=argparse.BooleanOptionalAction, default=True)

    args = parser.parse_args()
    
    config = VLMConfig(
        model_path=args.model_path,
        base_model_id=args.base_model_id,
        image_folder=args.image_folder,
        test_data_path=args.test_data_path,
        output_dir=args.output_dir,
        use_previous_context=args.use_context
    )
    
    runner = WebtoonVLM(config)
    
    if args.mode == 'predict':
        results = runner.run(mode='predict')
        print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {Path(args.output_dir) / config.predictions_file}")
        print(f"ğŸ“Š ì´ {len(results)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ")
        
    elif args.mode == 'evaluate':
        metrics = runner.run(mode='evaluate')
        print("\nâœ… í‰ê°€ ì™„ë£Œ!")
        print("ğŸ“Š ì£¼ìš” í‰ê°€ ì§€í‘œ:")
        if metrics:
            for metric, value in metrics.items():
                print(f"- {metric}: {value:.4f}")
        print(f"ğŸ“ ìƒì„¸ ê²°ê³¼ íŒŒì¼: {Path(args.output_dir) / config.metrics_file}")

if __name__ == "__main__":
    main()