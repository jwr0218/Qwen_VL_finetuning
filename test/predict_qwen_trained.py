"""
Vision Language Model ÏòàÏ∏° Î∞è ÌèâÍ∞Ä ÌÜµÌï© Ïä§ÌÅ¨Î¶ΩÌä∏ (ÏõπÌà∞ ÏàúÏ∞® Ï∂îÎ°† Î∞è OCR ÌèâÍ∞Ä ÏßÄÏõê)

Ïù¥ Î™®ÎìàÏùÄ ÌååÏù∏ÌäúÎãùÎêú Qwen2.5-VL Î™®Îç∏Ïùò ÏòàÏ∏° Î∞è ÌèâÍ∞ÄÎ•º Îã®Ïùº ÌÅ¥ÎûòÏä§ ÎÇ¥ÏóêÏÑú Íµ¨ÌòÑÌï©ÎãàÎã§.
'predict' Î™®ÎìúÏóêÏÑúÎäî ÏõπÌà∞ Ïù¥ÎØ∏ÏßÄÎ•º ÏàúÏ∞®Ï†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÎ©∞ Ïª®ÌÖçÏä§Ìä∏Î•º ÎàÑÏ†Å Ï≤òÎ¶¨ÌïòÍ≥†,
'evaluate' Î™®ÎìúÏóêÏÑúÎäî ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Î∞è OCR/Detection ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§.

ÏûëÏÑ±Ïûê: Assistant
Year: 2025
Month: 9
Day: 17
(Refactored on: 2025-10-16)
"""
import gc
import json
import logging
import os
import re
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
from datasets import load_dataset
from nltk import download, word_tokenize
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from PIL import Image
from qwen_vl_utils import process_vision_info
# ÌèâÍ∞Ä ÏßÄÌëú Í¥ÄÎ†® ÎùºÏù¥Î∏åÎü¨Î¶¨
from rouge import Rouge
from bert_score import score as bert_score
from tqdm import tqdm
from transformers import (AutoProcessor,
                          Qwen2_5_VLForConditionalGeneration)

# NLTK Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú (ÏµúÏ¥à Ïã§Ìñâ Ïãú)
try:
    download('punkt', quiet=True)
    download('wordnet', quiet=True)
except Exception:
    pass

@dataclass
class VLMConfig:
    """VLM ÏòàÏ∏° Î∞è ÌèâÍ∞Ä ÏÑ§Ï†ïÏùÑ ÌÜµÌï© Í¥ÄÎ¶¨ÌïòÎäî Îç∞Ïù¥ÌÑ∞ÌÅ¥ÎûòÏä§"""
    
    # üìÅ Îç∞Ïù¥ÌÑ∞ Î∞è Î™®Îç∏ Í≤ΩÎ°ú
    model_path: str = ''
    base_model_id: str = "Qwen/Qwen2-VL-7B-Instruct" 

    # ‚û°Ô∏è ÏòàÏ∏° Î™®Îìú ÏÑ§Ï†ï
    image_folder: str = '/workspace/Toonspace_VLM/webtoon_images'
    use_previous_context: bool = False
    max_context_length: int = 2000
    context_summary_length: int = 500
    image_extensions: List[str] = field(default_factory=lambda: ['.jpg', '.jpeg', '.png', '.webp'])
    # base_prompt: str = "Ïù¥ ÏõπÌà∞ Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÏÑùÌïòÏó¨ ÎåÄÏÇ¨, Ìö®Í≥ºÏùå, Ï∫êÎ¶≠ÌÑ∞ ÌñâÎèô, Í∞êÏ†ïÏùÑ JSON ÌòïÏãùÏúºÎ°ú Ï∂îÏ∂úÌï¥Ï£ºÏÑ∏Ïöî."
#     context_prompt_template: str = """Ïù¥Ï†Ñ Ïû•Î©¥ ÏöîÏïΩ:
# {previous_context}

# ÌòÑÏû¨ Ïù¥ÎØ∏ÏßÄÎ•º ÏúÑÏùò Ïª®ÌÖçÏä§Ìä∏Î•º Í≥†Î†§ÌïòÏó¨ Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî. Ïù¥ ÏõπÌà∞ Ïù¥ÎØ∏ÏßÄÏùò ÎåÄÏÇ¨, Ìö®Í≥ºÏùå, Ï∫êÎ¶≠ÌÑ∞ ÌñâÎèô, Í∞êÏ†ïÏùÑ JSON ÌòïÏãùÏúºÎ°ú Ï∂îÏ∂úÌï¥Ï£ºÏÑ∏Ïöî."""

    # üìä ÌèâÍ∞Ä Î™®Îìú ÏÑ§Ï†ï
    test_data_path: str = '/workspace/Toonspace_VLM/test/OCR_test_dataset.json'
    compute_bertscore: bool = True
    compute_json_metrics: bool = True
    
    ##### NEW/MODIFIED START #####
    compute_ocr_metrics: bool = True # OCR/Detection Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞ Ïó¨Î∂Ä
    iou_threshold: float = 0.5       # IoU ÏûÑÍ≥ÑÍ∞í
    ##### NEW/MODIFIED END #####

    # ‚öôÔ∏è Î™®Îç∏ Î∞è ÏÉùÏÑ± Í≥µÌÜµ ÏÑ§Ï†ï
    output_dir: str = 'results'
    predictions_file: str = 'predictions.json'
    metrics_file: str = 'evaluation_metrics.json'
    batch_size: int = 1
    max_samples: Optional[int] = None
    max_new_tokens: int = 1024
    temperature: float = 0.7
    top_p: float = 0.9
    do_sample: bool = True
    min_pixels: int = 256 * 28 * 28
    max_pixels: int = 960 * 28 * 28

    # üìù ÏãúÏä§ÌÖú Î©îÏãúÏßÄ
    system_message: str = field(default="""
    ÎãπÏã†ÏùÄ ÏõπÌà∞ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. ÏõπÌà∞Ïùò Ïó∞ÏÜçÎêú Ïû•Î©¥ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Ïä§ÌÜ†Î¶¨Ïùò ÌùêÎ¶ÑÏùÑ Ïù¥Ìï¥ÌïòÍ≥†,
    Í∞Å Ïû•Î©¥Ïùò ÎåÄÏÇ¨, Ìö®Í≥ºÏùå, Ï∫êÎ¶≠ÌÑ∞Ïùò ÌñâÎèôÍ≥º Í∞êÏ†ï, ÏÑúÏÇ¨Ï†Å Îß•ÎùΩÏùÑ Ï†ïÌôïÌûà Ï∂îÏ∂úÌï©ÎãàÎã§.
    Ïù¥Ï†Ñ Ïû•Î©¥Ïùò Îß•ÎùΩÏùÑ Í≥†Î†§ÌïòÏó¨ ÌòÑÏû¨ Ïû•Î©¥ÏùÑ Îçî Ï†ïÌôïÌïòÍ≤å Ìï¥ÏÑùÌïòÍ≥†,
    Î™®Îì† Í≤∞Í≥ºÎ•º Íµ¨Ï°∞ÌôîÎêú JSON ÌòïÏãùÏúºÎ°ú Ï†úÍ≥µÌï©ÎãàÎã§.
    """)

class WebtoonVLM:
    """ÏõπÌà∞ VLM ÏòàÏ∏° Î∞è ÌèâÍ∞ÄÎ•º ÏúÑÌïú ÌÜµÌï© ÌÅ¥ÎûòÏä§"""

    def __init__(self, config: VLMConfig):
        self.config = config
        self.setup_logging()
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.processor = None
        self.rouge = Rouge()
        
        self.load_model_and_processor()

    def setup_logging(self) -> None:
        """Î°úÍπÖ ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(Path(self.config.output_dir) / 'vlm_runner.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def load_model_and_processor(self) -> None:
        """ÌååÏù∏ÌäúÎãùÎêú Î™®Îç∏ Î∞è ÌîÑÎ°úÏÑ∏ÏÑú Î°úÎìú"""
        if self.model is not None:
            self.logger.info("Î™®Îç∏Ïù¥ Ïù¥ÎØ∏ Î°úÎìúÎêòÏóàÏäµÎãàÎã§.")
            return
        try:
            self.logger.info(f"Î™®Îç∏ Î°úÎìú ÏãúÏûë: {self.config.model_path}")
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                self.config.model_path, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True,
            )
            self.processor = AutoProcessor.from_pretrained(
                self.config.base_model_id, min_pixels=self.config.min_pixels, max_pixels=self.config.max_pixels, trust_remote_code=True
            )
            self.model.eval()
            self.logger.info("Î™®Îç∏ Î∞è ÌîÑÎ°úÏÑ∏ÏÑú Î°úÎìú ÏôÑÎ£å")
        except Exception as e:
            self.logger.error(f"Î™®Îç∏ Î°úÎìú Ï§ë ÏπòÎ™ÖÏ†Å Ïò§Î•ò Î∞úÏÉù: {e}")
            raise

    def clear_memory(self) -> None:
        """GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
    def run(self, mode: str):
        """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò: Î™®ÎìúÏóê Îî∞Îùº ÏòàÏ∏° ÎòêÎäî ÌèâÍ∞Ä Ïã§Ìñâ"""
        if mode == 'predict':
            return self._run_prediction()
        elif mode == 'evaluate':
            return self._run_evaluation()
        else:
            self.logger.error(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î™®ÎìúÏûÖÎãàÎã§: {mode}. 'predict' ÎòêÎäî 'evaluate'Î•º ÏÑ†ÌÉùÌï¥Ï£ºÏÑ∏Ïöî.")
            raise ValueError(f"Invalid mode: {mode}")
    
    # ... (Ïù¥Ï†Ñ ÏΩîÎìúÏùò ÏòàÏ∏° Î™®Îìú Í¥ÄÎ†® Î©îÏÑúÎìúÎì§ÏùÄ Î≥ÄÍ≤ΩÎêòÏßÄ ÏïäÏïòÏúºÎØÄÎ°ú ÏÉùÎûµ) ...
    def _run_prediction(self) -> List[Dict[str, Any]]:
        folder_path = self.config.image_folder
        self.logger.info(f"Ìè¥Îçî ÏòàÏ∏° Î™®Îìú ÏãúÏûë: {folder_path}")
        image_files = self._get_sorted_image_files(folder_path)
        if not image_files: return []
        all_results, context_history = [], []
        for idx, image_path in enumerate(tqdm(image_files, desc="Processing images")):
            prompt = self._build_context_prompt(context_history)
            result = self._predict_single(image_path=image_path, query=prompt)
            result['sequence_number'] = idx + 1
            result['used_context'] = self.config.use_previous_context and bool(context_history)
            if self.config.use_previous_context:
                context_summary = self._extract_key_info_from_prediction(result['prediction'])
                context_history.append(context_summary)
                if len(context_history) > 5: context_history.pop(0)
            all_results.append(result)
            if (idx + 1) % 10 == 0: self.clear_memory()
        self._save_prediction_results(all_results, folder_path)
        self.logger.info(f"ÏòàÏ∏° ÏôÑÎ£å: Ï¥ù {len(all_results)}Í∞ú Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨")
        return all_results
    def _get_sorted_image_files(self, folder_path: str) -> List[Path]:
        folder = Path(folder_path)
        if not folder.is_dir(): raise FileNotFoundError(f"Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {folder_path}")
        image_files = [p for p in folder.iterdir() if p.suffix.lower() in self.config.image_extensions]
        def natural_sort_key(path): return [int(c) if c.isdigit() else c.lower() for c in re.split(r'(\d+)', path.name)]
        image_files.sort(key=natural_sort_key)
        self.logger.info(f"Ï¥ù {len(image_files)}Í∞úÏùò Ïù¥ÎØ∏ÏßÄ ÌååÏùº Î∞úÍ≤¨")
        return image_files
    def _build_context_prompt(self, context_history: List[str]) -> str:
        if not self.config.use_previous_context or not context_history: return self.config.base_prompt
        context_summary = "\n".join([f"Ïû•Î©¥ {i+1}: {ctx}" for i, ctx in enumerate(context_history)])
        if len(self.processor.tokenizer.encode(context_summary)) > self.config.max_context_length:
            context_summary = context_summary[-self.config.max_context_length:]
        return self.config.context_prompt_template.format(previous_context=context_summary)
    def _extract_key_info_from_prediction(self, prediction: str) -> str:
        try:
            pred_json = json.loads(prediction)
            summary_parts = []
            if 'dialogues' in pred_json and pred_json['dialogues']: summary_parts.append(f"ÎåÄÏÇ¨: {', '.join([d.get('text', '') for d in pred_json['dialogues'][:2]])}")
            if 'characters' in pred_json and pred_json['characters']: summary_parts.append(f"ÌñâÎèô: {', '.join([c.get('action', '') for c in pred_json['characters'][:2] if c.get('action')])}")
            return ' | '.join(summary_parts) if summary_parts else prediction[:self.config.context_summary_length]
        except (json.JSONDecodeError, TypeError): return prediction[:self.config.context_summary_length]
    def _save_prediction_results(self, results: List[Dict], source_folder: str):
        output_data = {'metadata': {'source_folder': source_folder, 'model_path': self.config.model_path, 'total_images': len(results), 'timestamp': datetime.now().isoformat(), 'config': self.config.__dict__}, 'predictions': results}
        output_path = Path(self.config.output_dir) / self.config.predictions_file
        with open(output_path, 'w', encoding='utf-8') as f: json.dump(output_data, f, ensure_ascii=False, indent=2)
        pd.DataFrame(results).to_csv(Path(self.config.output_dir) / 'predictions.csv', index=False, encoding='utf-8-sig')
        self.logger.info(f"ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å: {output_path}")

    # ===================================================================
    # ÌèâÍ∞Ä Î™®Îìú (Evaluation Mode) - ÏàòÏ†ïÎê®
    # evaluate - query : OCR Îßå ÏÇ¨Ïö©. 
    # ===================================================================

    def _run_evaluation(self) -> Dict[str, Any]:
        """Î™®Îç∏ ÏòàÏ∏° Î∞è ÌèâÍ∞Ä ÏßÄÌëú Í≥ÑÏÇ∞"""
        self.logger.info("ÌèâÍ∞Ä Î™®Îìú ÏãúÏûë...")
        test_data = self._load_test_data()
        results_with_metadata = []
        
        start_time = time.time()
        for idx, sample in enumerate(tqdm(test_data, desc="Evaluating")):
            try:
                query = sample['query']
                result = self._predict_single(image_path=sample['image_path'], query=query)
                result['ground_truth'] = sample['answer']
                results_with_metadata.append(result)
            except Exception as e:
                self.logger.error(f"ÏÉòÌîå {idx+1} ({sample.get('image_path')}) Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
                results_with_metadata.append({"image_path": sample.get('image_path'), "prediction": "[ERROR]", "ground_truth": sample.get('answer'), "error": str(e)})
            if (idx + 1) % 10 == 0: self.clear_memory()

        total_time = time.time() - start_time
        self.logger.info(f"ÌèâÍ∞Ä ÏôÑÎ£å. Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ: {total_time:.2f}Ï¥à")

        predictions = [r['prediction'] for r in results_with_metadata]
        ground_truths = [r['ground_truth'] for r in results_with_metadata]
        
        metrics = self._calculate_all_metrics(predictions, ground_truths)
        self._save_evaluation_results(metrics, results_with_metadata)
        
        return metrics

    def _load_test_data(self) -> List[Dict]:
        """ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú"""
        try:
            dataset = load_dataset('json', data_files=self.config.test_data_path)['train']
            # dataset = dataset.select(range(20))
            if self.config.max_samples:
                dataset = dataset.select(range(min(self.config.max_samples, len(dataset))))
            self.logger.info(f"ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å: {len(dataset)}Í∞ú ÏÉòÌîå")
            return dataset
        except Exception as e:
            self.logger.error(f"Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë Ïò§Î•ò: {e}")
            raise

    ##### NEW/MODIFIED START #####
    def _parse_ocr_string(self, ocr_str: str) -> List[Dict[str, Union[str, List[float]]]]:
        """ 'TEXT : [x1, y1, x2, y2]' ÌòïÏãùÏùò Î¨∏ÏûêÏó¥ÏùÑ ÌååÏã±Ìï©ÎãàÎã§. """
        entries = []
        if not isinstance(ocr_str, str):
            return []
        for line in ocr_str.strip().split('\n'):
            try:
                parts = line.split(' : ')
                if len(parts) == 2:
                    text = parts[0]
                    # ÎåÄÍ¥ÑÌò∏ ÏïàÏùò ÎÇ¥Ïö©Îßå Ï∂îÏ∂úÌïòÏó¨ ÌååÏã±
                    bbox_str = parts[1].strip()
                    bbox = json.loads(bbox_str)
                    if len(bbox) == 4:
                        entries.append({'text': text, 'box': [float(coord) for coord in bbox]})
            except (json.JSONDecodeError, IndexError, ValueError) as e:
                self.logger.debug(f"OCR Î¨∏ÏûêÏó¥ ÌååÏã± Ïã§Ìå®: '{line}'. Ïò§Î•ò: {e}")
                continue
        return entries

    def _calculate_iou(self, boxA: List[float], boxB: List[float]) -> float:
        """Îëê Î∞îÏö¥Îî© Î∞ïÏä§ Í∞ÑÏùò IoU(Intersection over Union)Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§."""
        xA = max(boxA[0], boxB[0])
        yA = max(boxA[1], boxB[1])
        xB = min(boxA[2], boxB[2])
        yB = min(boxA[3], boxB[3])

        interArea = max(0, xB - xA) * max(0, yB - yA)
        boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
        boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
        
        unionArea = float(boxAArea + boxBArea - interArea)
        return interArea / unionArea if unionArea > 0 else 0.0
    def _compute_map(self, predictions: List[str], ground_truths: List[str], iou_threshold: float) -> float:
        """
        Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ÑÏ≤¥Ïóê ÎåÄÌïú Average Precision (AP)ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.
        Ï∞∏Í≥†: Ïã†Î¢∞ÎèÑ Ï†êÏàòÍ∞Ä ÏóÜÏúºÎØÄÎ°ú Î™®Îì† ÏòàÏ∏°Ïùò Ïã†Î¢∞ÎèÑÎ•º 1.0ÏúºÎ°ú Í∞ÄÏ†ïÌï©ÎãàÎã§.
        """
        all_preds = []
        total_gt_boxes = 0

        # 1. Î™®Îì† ÏòàÏ∏°Í≥º Ï†ïÎãµÏùÑ ÌïòÎÇòÏùò Î¶¨Ïä§Ìä∏Î°ú ÌÜµÌï©
        for i, (pred_str, gt_str) in enumerate(zip(predictions, ground_truths)):
            pred_boxes = self._parse_ocr_string(pred_str)
            gt_boxes = self._parse_ocr_string(gt_str)
            total_gt_boxes += len(gt_boxes)
            
            # Í∞Å ÏòàÏ∏°Ïóê Ïù¥ÎØ∏ÏßÄ Ïù∏Îç±Ïä§ÏôÄ Ïã†Î¢∞ÎèÑ(Í∞ÄÏßú) Ï∂îÍ∞Ä
            for pred_box in pred_boxes:
                all_preds.append({'image_id': i, 'confidence': 1.0, 'box': pred_box})
        
        # Ïã†Î¢∞ÎèÑ Ï†êÏàò Í∏∞Ï§Ä ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨ (ÌòÑÏû¨Îäî Î™®Îëê 1.0Ïù¥Îùº ÏùòÎØ∏Îäî ÏóÜÏßÄÎßå, ÏïåÍ≥†Î¶¨Ï¶òÏÉÅ ÌïÑÏöî)
        all_preds.sort(key=lambda x: x['confidence'], reverse=True)

        # 2. Í∞Å ÏòàÏ∏°Ïóê ÎåÄÌï¥ TP/FP ÌåêÎ≥Ñ
        gt_matched_map = {i: [False] * len(self._parse_ocr_string(gt_str)) for i, gt_str in enumerate(ground_truths)}
        
        tp_fp_list = []
        for pred in all_preds:
            image_id = pred['image_id']
            gt_boxes = self._parse_ocr_string(ground_truths[image_id])
            
            best_iou = 0
            best_gt_idx = -1
            for i, gt_box in enumerate(gt_boxes):
                if not gt_matched_map[image_id][i]:
                    iou = self._calculate_iou(pred['box']['box'], gt_box['box'])
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = i

            if best_gt_idx != -1 and best_iou >= iou_threshold and \
               pred['box']['text'].strip() == gt_boxes[best_gt_idx]['text'].strip():
                gt_matched_map[image_id][best_gt_idx] = True
                tp_fp_list.append(1)  # True Positive
            else:
                tp_fp_list.append(0)  # False Positive

        # 3. Precision-Recall Í≥°ÏÑ† Í≥ÑÏÇ∞
        if total_gt_boxes == 0:
            return 0.0
            
        tp_cumsum = np.cumsum(tp_fp_list)
        precision_curve = [tp_cumsum[i] / (i + 1) for i in range(len(tp_fp_list))]
        recall_curve = [tp_cumsum[i] / total_gt_boxes for i in range(len(tp_fp_list))]

        # 4. AP (Area under curve) Í≥ÑÏÇ∞
        precision_curve = np.array([0.0] + precision_curve)
        recall_curve = np.array([0.0] + recall_curve)
        
        # Ïö∞ÌïòÌñ• Í≥°ÏÑ†ÏúºÎ°ú Î≥¥Ï†ï
        for i in range(len(precision_curve) - 2, -1, -1):
            precision_curve[i] = max(precision_curve[i], precision_curve[i+1])

        # recall Í∞íÏù¥ Î≥ÄÌïòÎäî ÏßÄÏ†êÎßå ÏÑ†ÌÉùÌïòÏó¨ Î©¥Ï†Å Í≥ÑÏÇ∞
        recall_change_indices = np.where(recall_curve[1:] != recall_curve[:-1])[0]
        ap = np.sum((recall_curve[recall_change_indices + 1] - recall_curve[recall_change_indices]) * precision_curve[recall_change_indices + 1])
        
        return ap
    def _compute_detection_metrics(self, predictions: List[str], ground_truths: List[str]) -> Dict[str, float]:
        """OCR Î∞è BBox Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú Precision, Recall, F1-score, Avg IoU, mAPÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§."""
        total_tp, total_fp, total_fn = 0, 0, 0
        iou_scores_for_tps = [] # TPÎì§Ïùò IoU Ï†êÏàòÎ•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏

        for pred_str, gt_str in zip(predictions, ground_truths):
            pred_boxes = self._parse_ocr_string(pred_str)
            gt_boxes = self._parse_ocr_string(gt_str)

            if not gt_boxes and not pred_boxes:
                continue

            tp = 0
            gt_matched = [False] * len(gt_boxes)

            # Í∞Å ÏòàÏ∏° Î∞ïÏä§Î•º ÏµúÏ†ÅÏùò Ï†ïÎãµ Î∞ïÏä§ÏôÄ Îß§Ïπ≠
            for pred_box in pred_boxes:
                best_iou = 0
                best_gt_idx = -1
                for i, gt_box in enumerate(gt_boxes):
                    if gt_matched[i]: continue
                    iou = self._calculate_iou(pred_box['box'], gt_box['box'])
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = i

                # TP Ï°∞Í±¥: IoU ÏûÑÍ≥ÑÍ∞í Ï¥àÍ≥º Î∞è ÌÖçÏä§Ìä∏ ÏùºÏπò
                if best_gt_idx != -1 and best_iou >= self.config.iou_threshold and \
                   pred_box['text'].strip() == gt_boxes[best_gt_idx]['text'].strip():
                    tp += 1
                    gt_matched[best_gt_idx] = True
                    iou_scores_for_tps.append(best_iou) # TPÏùº ÎïåÏùò IoU Ï†êÏàò Ï†ÄÏû•
            
            fp = len(pred_boxes) - tp
            fn = len(gt_boxes) - sum(gt_matched)

            total_tp += tp
            total_fp += fp
            total_fn += fn

        # --- Í∏∞Î≥∏ Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞ ---
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # --- ÌèâÍ∑† IoU Í≥ÑÏÇ∞ ---
        average_iou = np.mean(iou_scores_for_tps) if iou_scores_for_tps else 0.0

        # --- mAP Í≥ÑÏÇ∞ ---
        # ÌÅ¥ÎûòÏä§Í∞Ä ÌïòÎÇòÏù¥ÎØÄÎ°ú mAP@0.5Îäî AP@0.5ÏôÄ ÎèôÏùºÌï©ÎãàÎã§.
        map_score = self._compute_map(predictions, ground_truths, self.config.iou_threshold)

        return {
            'ocr_precision': precision,
            'ocr_recall': recall,
            'ocr_f1_score': f1_score,
            'average_iou': average_iou,
            f'mAP_at_{self.config.iou_threshold}': map_score
        }

    def _calculate_all_metrics(self, predictions: List[str], ground_truths: List[str]) -> Dict[str, Any]:
        """Î™®Îì† ÌèâÍ∞Ä ÏßÄÌëúÎ•º Í≥ÑÏÇ∞ÌïòÍ≥† ÌÜµÌï©"""
        metrics = {}
        valid_pairs = [(p, r) for p, r in zip(predictions, ground_truths) if p and r and p != "[ERROR]"]
        if not valid_pairs:
            self.logger.warning("Í≥ÑÏÇ∞Ìï† Ïú†Ìö®Ìïú ÏòàÏ∏°-Ï†ïÎãµ ÏåçÏù¥ ÏóÜÏäµÎãàÎã§.")
            return {"error": "No valid prediction-reference pairs found."}
            
        filtered_preds, filtered_refs = zip(*valid_pairs)
        
        # Text Metrics (Í∏∞Ï°¥)
        try:
            rouge_scores = self.rouge.get_scores(filtered_preds, filtered_refs, avg=True)
            metrics['rouge-l'] = rouge_scores['rouge-l']['f']
        except Exception as e: self.logger.warning(f"ROUGE Í≥ÑÏÇ∞ Ï§ë Ïò§Î•ò: {e}")
        
        # BERTScore (Í∏∞Ï°¥)
        if self.config.compute_bertscore:
            try:
                _, _, F1 = bert_score(list(filtered_preds), list(filtered_refs), lang="ko", device=self.device)
                metrics['bert_score_f1'] = F1.mean().item()
            except Exception as e: self.logger.warning(f"BERTScore Í≥ÑÏÇ∞ Ï§ë Ïò§Î•ò: {e}")

        # JSON Metrics (Í∏∞Ï°¥)
        if self.config.compute_json_metrics:
            # Ïù¥ Î∂ÄÎ∂ÑÏùÄ ÌòÑÏû¨ OCR ÌèâÍ∞ÄÏôÄÎäî Î¨¥Í¥ÄÌïòÎØÄÎ°ú, ÌïÑÏöî Ïãú JSON ÌòïÏãùÏóê ÎßûÍ≤å ÏàòÏ†ï ÌïÑÏöî
            pass

        # OCR/Detection Metrics (Ïã†Í∑ú)
        if self.config.compute_ocr_metrics:
            self.logger.info(f"OCR/Detection Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞ (IoU Threshold: {self.config.iou_threshold})...")
            detection_metrics = self._compute_detection_metrics(predictions, ground_truths)
            metrics.update(detection_metrics)

        return metrics
    ##### NEW/MODIFIED END #####

    def _save_evaluation_results(self, metrics: Dict, results_with_metadata: List[Dict]):
        """ÌèâÍ∞Ä Í≤∞Í≥º Î∞è Î©îÌä∏Î¶≠ Ï†ÄÏû•"""
        metadata = {"timestamp": datetime.now().isoformat(), "model_path": self.config.model_path, "test_data_path": self.config.test_data_path}
        
        metrics_output = {"metadata": metadata, "metrics": metrics}
        metrics_path = Path(self.config.output_dir) / self.config.metrics_file
        with open(metrics_path, 'w', encoding='utf-8') as f: json.dump(metrics_output, f, ensure_ascii=False, indent=4)
        self.logger.info(f"ÌèâÍ∞Ä Î©îÌä∏Î¶≠ Ï†ÄÏû• ÏôÑÎ£å: {metrics_path}")

        predictions_output = {"metadata": metadata, "predictions": results_with_metadata}
        predictions_path = Path(self.config.output_dir) / self.config.predictions_file
        with open(predictions_path, 'w', encoding='utf-8') as f: json.dump(predictions_output, f, ensure_ascii=False, indent=2)
        self.logger.info(f"Í∞úÎ≥Ñ ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å: {predictions_path}")

    @torch.no_grad()
    def _predict_single(self, image_path: Union[str, Path], query: str) -> Dict[str, Any]:
        """Îã®Ïùº Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú ÏòàÏ∏°ÏùÑ ÏàòÌñâÌïòÎäî Í≥µÌÜµ Ìï®Ïàò"""
        try:
            image = Image.open(image_path).convert('RGB')
            messages = [{"role": "system", "content": self.config.system_message}, {"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": query}]}]
            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, _ = process_vision_info(messages)
            inputs = self.processor(text=[text], images=image_inputs, padding=True, return_tensors="pt").to(self.device)
            start_time = time.time()
            gen_ids = self.model.generate(**inputs, max_new_tokens=self.config.max_new_tokens, temperature=self.config.temperature, top_p=self.config.top_p, do_sample=self.config.do_sample)
            gen_time = time.time() - start_time
            gen_ids = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen_ids)]
            prediction = self.processor.batch_decode(gen_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
            return {'image_path': str(image_path), 'image_name': Path(image_path).name, 'query': query, 'prediction': prediction, 'generation_time': gen_time}
        except Exception as e:
            self.logger.error(f"Ïù¥ÎØ∏ÏßÄ {image_path} ÏòàÏ∏° Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            return {'image_path': str(image_path), 'image_name': Path(image_path).name, 'query': query, 'prediction': "[PREDICTION_ERROR]", 'error': str(e)}

def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    import argparse
    
    parser = argparse.ArgumentParser(description='VLM Î™®Îç∏ ÏòàÏ∏° Î∞è ÌèâÍ∞Ä ÌÜµÌï© Ïä§ÌÅ¨Î¶ΩÌä∏')
    parser.add_argument('--mode', type=str, default='evaluate', choices=['predict', 'evaluate'])
    parser.add_argument('--model_path', type=str, default='/workspace/Toonspace_VLM/ex_models/at_once_ocr_description')
    parser.add_argument('--base_model_id', type=str, default='huihui-ai/Qwen2.5-VL-7B-Instruct-abliterated')
    parser.add_argument('--image_folder', type=str, default='/workspace/Toonspace_VLM/data/test_image/escape_home/01')
    parser.add_argument('--test_data_path', type=str, default='/workspace/Toonspace_VLM/test/OCR_test_dataset.json')
    parser.add_argument('--output_dir', type=str, default='results')
    parser.add_argument('--use_context', action=argparse.BooleanOptionalAction, default=True)

    args = parser.parse_args()
    
    config = VLMConfig(
        model_path=args.model_path,
        base_model_id=args.base_model_id,
        image_folder=args.image_folder,
        test_data_path=args.test_data_path,
        output_dir=args.output_dir,
        use_previous_context=args.use_context
    )
    
    runner = WebtoonVLM(config)
    
    if args.mode == 'predict':
        results = runner.run(mode='predict')
        print(f"\n‚úÖ ÏòàÏ∏° ÏôÑÎ£å!")
        print(f"üìÅ Í≤∞Í≥º ÌååÏùº: {Path(args.output_dir) / config.predictions_file}")
        print(f"üìä Ï¥ù {len(results)}Í∞ú Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÏôÑÎ£å")
        
    elif args.mode == 'evaluate':
        metrics = runner.run(mode='evaluate')
        print("\n‚úÖ ÌèâÍ∞Ä ÏôÑÎ£å!")
        print("üìä Ï£ºÏöî ÌèâÍ∞Ä ÏßÄÌëú:")
        if metrics:
            for metric, value in metrics.items():
                print(f"- {metric}: {value:.4f}")
        print(f"üìÅ ÏÉÅÏÑ∏ Í≤∞Í≥º ÌååÏùº: {Path(args.output_dir) / config.metrics_file}")

if __name__ == "__main__":
    main()